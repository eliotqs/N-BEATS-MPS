{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eliotqs/N-BEATS-MPS/blob/main/N_BEATS_MPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkHDF2j93u6I"
      },
      "source": [
        "## Improving FC stability using DL - Thesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFVgi0WfzpF1"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "\n",
        "#!pip install wandb\n",
        "\n",
        "import wandb #weights and biases\n",
        "\n",
        "# For earlystopping:\n",
        "#from google.colab import files\n",
        "#files.upload()\n",
        "#import pytorchtools\n",
        "#from pytorchtools import EarlyStopping\n",
        "\n",
        "from itertools import repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDuMsy7CzmjY"
      },
      "source": [
        "# **Data loading & data preprocessing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# M3"
      ],
      "metadata": {
        "id": "1_k6e5DZOIaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#M3\n",
        "#from google.colab import files\n",
        "#full_data = files.upload() #upload M3 dataset\n",
        "#df = pd.read_excel('/content/M3C.xls')\n",
        "#df.set_index('Series', inplace = True)\n",
        "\n",
        "# Get the data in numpy representation\n",
        "#testset_np = df.values\n",
        "# Select all non NaN values from the trainset\n",
        "#testset_clean = [x[x == x] for x in testset_np]\n",
        "#valset_np = [x[:-18] for x in testset_clean]\n",
        "# Train/validation/test --------------------------------- NBeats paper validation strategy\n",
        "#testset_m4m = [x[x == x] for x in testset_np]\n",
        "#valset_m4m = valset_np\n",
        "#trainset_m4m = [x[:-18] for x in valset_np]\n",
        "\n",
        "#del(testset_np, testset_clean, valset_np)"
      ],
      "metadata": {
        "id": "qJZLrj6vONFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# M4"
      ],
      "metadata": {
        "id": "YEsjXLNvOM49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load monthly M4 data.\n",
        "# Transform the data into a lists of arrays. Each inner array represents a timeseries.\n",
        "# Remove all the NaN values from the datasets.\n",
        "\n",
        "# M4\n",
        "trainset = pd.read_csv('https://raw.githubusercontent.com/Mcompetitions/M4-methods/master/Dataset/Train/Monthly-train.csv')\n",
        "testset = pd.read_csv('https://raw.githubusercontent.com/Mcompetitions/M4-methods/master/Dataset/Test/Monthly-test.csv')\n",
        "trainset.set_index('V1', inplace = True)\n",
        "testset.set_index('V1', inplace = True)\n",
        "# Add the testset columns behind the trainset columns\n",
        "testset_merge = trainset.merge(testset, on = 'V1', how = 'inner')\n",
        "# Get the data in numpy representation\n",
        "trainset_np = trainset.values\n",
        "testset_np = testset_merge.values\n",
        "# Select all non NaN values from the trainset\n",
        "trainset_clean = [x[x == x] for x in trainset_np]\n",
        "# Train/validation/test --------------------------------- NBeats paper validation strategy\n",
        "testset_m4m = [x[x == x] for x in testset_np]\n",
        "valset_m4m = trainset_clean.copy()\n",
        "trainset_m4m = [x[:-18] for x in trainset_clean]\n",
        "\n",
        "del(trainset, testset, testset_merge, trainset_np, testset_np, trainset_clean)"
      ],
      "metadata": {
        "id": "e7LvRYIwsu6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4WHrQ_9z4u7"
      },
      "source": [
        "# Scale independent loss functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6k5FSBDz88O"
      },
      "outputs": [],
      "source": [
        "# output = batch x fl\n",
        "# target = batch x fl\n",
        "# actuals_train = batch x bl\n",
        "\n",
        "def SMAPE(output, target, actuals_train = None):\n",
        "    \n",
        "    abs_errors = torch.abs(target - output)\n",
        "    abs_output = torch.abs(output)\n",
        "    abs_target = torch.abs(target)\n",
        "    loss = 200 * torch.mean(abs_errors / (abs_output.detach() + abs_target + 1e-5))\n",
        "    # possibly nan values in training networks if no offset in denominator is used\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "def MAPE(output, target, actuals_train = None):\n",
        "\n",
        "    abs_errors = torch.abs(target - output)\n",
        "    abs_target = torch.abs(target)\n",
        "    loss = 100 * torch.mean(abs_errors / (abs_target + 1e-5))\n",
        "    # possibly nan values in training networks if no offset in denominator is used\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def MASE(output, target, actuals_train): #Mean absolute scaled error\n",
        "    \n",
        "    mask = torch.abs(actuals_train)>1e-6\n",
        "    mad = torch.sum(torch.abs(actuals_train[:, 1:] - actuals_train[:, :-1]), dim = -1) / (torch.sum(mask, dim = -1) - 1)\n",
        "    mad_reshaped = mad.unsqueeze(-1).repeat_interleave(target.shape[-1], dim = -1)\n",
        "    loss_items = torch.mean((torch.abs(target - output)) / (mad_reshaped + 1e-5), dim = -1)\n",
        "    loss_items_clamped = torch.clamp(loss_items, 0, 5)\n",
        "    loss = torch.mean(loss_items_clamped)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def MASE_m(output, target, actuals_train):\n",
        "    \n",
        "    mask = torch.abs(actuals_train)>1e-6\n",
        "    mad = torch.sum(torch.abs(actuals_train[:, 12:] - actuals_train[:, :-12]), dim = -1) / (torch.sum(mask, dim = -1) - 12)\n",
        "    mad_reshaped = mad.unsqueeze(-1).repeat_interleave(target.shape[-1], dim = -1)\n",
        "    loss_items = torch.mean((torch.abs(target - output)) / (mad_reshaped + 1e-5), dim = -1)\n",
        "    loss_items_clamped = torch.clamp(loss_items, 0, 5)\n",
        "    loss = torch.mean(loss_items_clamped) \n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def RMSSE(output, target, actuals_train):\n",
        "    \n",
        "    mask = torch.abs(actuals_train)>1e-6\n",
        "    msd = torch.sum((actuals_train[:, 1:] - actuals_train[:, :-1])**2, dim = -1) / (torch.sum(mask, dim = -1) - 1)\n",
        "    msd_reshaped = msd.unsqueeze(-1).repeat_interleave(target.shape[-1], dim = -1)\n",
        "    loss_items = torch.sqrt(torch.mean((target - output)**2 / (msd_reshaped + 1e-5), dim = -1))\n",
        "    loss_items_clamped = torch.clamp(loss_items, 0, 5)\n",
        "    loss = torch.mean(loss_items_clamped)\n",
        "    #loss = torch.sqrt(torch.mean((target - output)**2 / msd_reshaped))\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def RMSSE_m(output, target, actuals_train):\n",
        "    \n",
        "    mask = torch.abs(actuals_train)>1e-6\n",
        "    msd = torch.sum((actuals_train[:, 12:] - actuals_train[:, :-12])**2, dim = -1) / (torch.sum(mask, dim = -1) - 12)\n",
        "    msd_reshaped = msd.unsqueeze(-1).repeat_interleave(target.shape[-1], dim = -1)\n",
        "    loss_items = torch.sqrt(torch.mean((target - output)**2 / (msd_reshaped + 1e-5), dim = -1))\n",
        "    loss_items_clamped = torch.clamp(loss_items, 0, 5)\n",
        "    loss = torch.mean(loss_items_clamped)\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDVSA91zz_Z6"
      },
      "source": [
        "#NBeats models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiZ7WzWb0C23"
      },
      "source": [
        "Generic building block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6ZywLgk0B37"
      },
      "outputs": [],
      "source": [
        "class GenericNBeatsBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 device,\n",
        "                 backcast_length,\n",
        "                 forecast_length,\n",
        "                 hidden_layer_units, thetas_dims, \n",
        "                 share_thetas,\n",
        "                 dropout = False, dropout_p = 0.0, \n",
        "                 neg_slope = 0.00):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.backcast_length = backcast_length\n",
        "        self.forecast_length = forecast_length        \n",
        "        if isinstance(hidden_layer_units, int):\n",
        "            self.hidden_layer_units = [hidden_layer_units for FC_layer in range(4)]\n",
        "        else:\n",
        "            #assert(len(hidden_layer_units) == 4)\n",
        "            self.hidden_layer_units = hidden_layer_units\n",
        "        self.thetas_dims = thetas_dims\n",
        "        self.share_thetas = share_thetas\n",
        "        self.dropout = dropout\n",
        "        self.dropout_p = dropout_p\n",
        "        self.neg_slope = neg_slope\n",
        "        \n",
        "        # shared layers in block\n",
        "        self.fc1 = nn.Linear(self.backcast_length,\n",
        "                             self.hidden_layer_units[0])#, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden_layer_units[0], self.hidden_layer_units[1])#, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden_layer_units[1], self.hidden_layer_units[2])#, bias = False)\n",
        "        self.fc4 = nn.Linear(self.hidden_layer_units[2], self.hidden_layer_units[3])#, bias = False)\n",
        "\n",
        "        \n",
        "        # do not use F.dropout as you want dropout to only affect training (not evaluation mode)\n",
        "        # nn.Dropout handles this automatically\n",
        "        if self.dropout:\n",
        "            self.dropoutlayer = nn.Dropout(p = self.dropout_p)\n",
        "        \n",
        "        # task specific (backcast & forecast) layers in block\n",
        "        # do not include bias - see section 3.1 - Ruben does include bias for generic blocks\n",
        "        if self.share_thetas:\n",
        "            self.theta_b_fc = self.theta_f_fc = nn.Linear(self.hidden_layer_units[3], self.thetas_dims)#, bias = False)\n",
        "        else:\n",
        "            self.theta_b_fc = nn.Linear(self.hidden_layer_units[3], self.thetas_dims)#, bias = False)\n",
        "            self.theta_f_fc = nn.Linear(self.hidden_layer_units[3], self.thetas_dims)#, bias = False)\n",
        "        \n",
        "        # block output layers\n",
        "        self.backcast_out = nn.Linear(self.thetas_dims, self.backcast_length)#, bias = False) # include bias - see section 3.3\n",
        "        self.forecast_out = nn.Linear(self.thetas_dims, self.forecast_length)#, bias = False) # include bias - see section 3.3\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        if self.dropout:\n",
        "            h1 = F.leaky_relu(self.fc1(x.to(self.device)), negative_slope = self.neg_slope)\n",
        "            h1 = self.dropoutlayer(h1)\n",
        "            h2 = F.leaky_relu(self.fc2(h1), negative_slope = self.neg_slope)\n",
        "            h2 = self.dropoutlayer(h2)\n",
        "            h3 = F.leaky_relu(self.fc3(h2), negative_slope = self.neg_slope)\n",
        "            h3 = self.dropoutlayer(h3)\n",
        "            h4 = F.leaky_relu(self.fc4(h3), negative_slope = self.neg_slope)\n",
        "            theta_b = F.leaky_relu(self.theta_b_fc(h4), negative_slope = self.neg_slope)\n",
        "            #theta_b = self.theta_b_fc(h4)\n",
        "            theta_f = F.leaky_relu(self.theta_f_fc(h4), negative_slope = self.neg_slope)\n",
        "            #theta_f = self.theta_f_fc(h4)\n",
        "            backcast = self.backcast_out(theta_b)\n",
        "            forecast = self.forecast_out(theta_f)\n",
        "        else:\n",
        "            h1 = F.leaky_relu(self.fc1(x.to(self.device)), negative_slope = self.neg_slope)\n",
        "            h2 = F.leaky_relu(self.fc2(h1), negative_slope = self.neg_slope)\n",
        "            h3 = F.leaky_relu(self.fc3(h2), negative_slope = self.neg_slope)\n",
        "            h4 = F.leaky_relu(self.fc4(h3), negative_slope = self.neg_slope)\n",
        "            theta_b = F.leaky_relu(self.theta_b_fc(h4), negative_slope = self.neg_slope)\n",
        "            #theta_b = self.theta_b_fc(h4)\n",
        "            theta_f = F.leaky_relu(self.theta_f_fc(h4), negative_slope = self.neg_slope)\n",
        "            #theta_f = self.theta_f_fc(h4)\n",
        "            backcast = self.backcast_out(theta_b)\n",
        "            forecast = self.forecast_out(theta_f)\n",
        "            \n",
        "        return backcast, forecast\n",
        "    \n",
        "    \n",
        "    def __str__(self):\n",
        "        \n",
        "        block_type = type(self).__name__\n",
        "        \n",
        "        return f'{block_type}(units={self.hidden_layer_units}, thetas_dims={self.thetas_dims}, ' \\\n",
        "            f'backcast_length={self.backcast_length}, ' \\\n",
        "            f'forecast_length={self.forecast_length}, share_thetas={self.share_thetas}, ' \\\n",
        "            f'dropout={self.dropout}, dropout_p={self.dropout_p}, neg_slope={self.neg_slope}) at @{id(self)}'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9FXcp_U0I_v"
      },
      "source": [
        "StableNBeatsNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0phMPY70Kgy"
      },
      "outputs": [],
      "source": [
        "# Only the forward method is changed compared to standard NBeatsNet\n",
        "class StableNBeatsNet(nn.Module): \n",
        "    \n",
        "    def __init__(self, \n",
        "                 device,\n",
        "                 backcast_length_multiplier,\n",
        "                 forecast_length,\n",
        "                 hidden_layer_units, thetas_dims, \n",
        "                 share_thetas,\n",
        "                 nb_blocks_per_stack, n_stacks, share_weights_in_stack,\n",
        "                 dropout = False, dropout_p = 0.0, \n",
        "                 neg_slope = 0.00):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.backcast_length = backcast_length_multiplier * forecast_length\n",
        "        self.forecast_length = forecast_length\n",
        "        self.hidden_layer_units = hidden_layer_units\n",
        "        self.thetas_dims = thetas_dims\n",
        "        self.share_thetas = share_thetas\n",
        "        self.nb_blocks_per_stack = nb_blocks_per_stack\n",
        "        self.n_stacks = n_stacks\n",
        "        self.share_weights_in_stack = share_weights_in_stack\n",
        "        self.dropout = dropout\n",
        "        self.dropout_p = dropout_p\n",
        "        self.neg_slope = neg_slope\n",
        "        \n",
        "        \n",
        "        self.stacks = []\n",
        "        self.parameters = []\n",
        "        \n",
        "        print(f'| N-Beats')\n",
        "        for stack_id in range(self.n_stacks):\n",
        "            self.stacks.append(self.create_stack(stack_id))\n",
        "        self.parameters = nn.ParameterList(self.parameters)\n",
        "        \n",
        "        \n",
        "    def create_stack(self, stack_id):\n",
        "        \n",
        "        print(f'| --  Stack Generic (#{stack_id}) (share_weights_in_stack={self.share_weights_in_stack})')\n",
        "        blocks = []\n",
        "        for block_id in range(self.nb_blocks_per_stack):\n",
        "            if self.share_weights_in_stack and block_id != 0:\n",
        "                block = blocks[-1]  # pick up the last one when we share weights\n",
        "            else:\n",
        "                block = GenericNBeatsBlock(self.device,\n",
        "                                           self.backcast_length,\n",
        "                                           self.forecast_length,\n",
        "                                           self.hidden_layer_units, self.thetas_dims, \n",
        "                                           self.share_thetas,\n",
        "                                           self.dropout, self.dropout_p, \n",
        "                                           self.neg_slope)\n",
        "                self.parameters.extend(block.parameters())\n",
        "                print(f'     | -- {block}')\n",
        "                blocks.append(block)\n",
        "                \n",
        "        return blocks\n",
        "\n",
        "    \n",
        "    def forward(self, backcast_arr):\n",
        "        \n",
        "        # dim backcast_arr = batch_size x shifts x backcast_length\n",
        "        # shifts == 0 is standard input window, others are shifted lookback windows \n",
        "        # higher index = further back in time\n",
        "        # feed different input windows (per batch) through the SAME network (check via list of learnable parameters)\n",
        "        # see https://stackoverflow.com/questions/54444630/application-of-nn-linear-layer-in-pytorch-on-additional-dimentions\n",
        "        \n",
        "        forecast_arr = torch.zeros((backcast_arr.shape[0], # take batch size from backcast\n",
        "                                    backcast_arr.shape[1], # take n of shifts from backcast\n",
        "                                    self.forecast_length), dtype = torch.float).to(self.device)\n",
        "        backcast_arr = backcast_arr.to(self.device)\n",
        "        \n",
        "        # loop through stacks (and blocks)\n",
        "        for stack_id in range(len(self.stacks)):\n",
        "            for block_id in range(len(self.stacks[stack_id])):\n",
        "                b, f = self.stacks[stack_id][block_id](backcast_arr)\n",
        "                backcast_arr = backcast_arr - b\n",
        "                forecast_arr = forecast_arr + f  \n",
        "                \n",
        "        return backcast_arr, forecast_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs9MkyM-0Nhm"
      },
      "source": [
        "#Training and evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoRnS-Ht0PbG"
      },
      "outputs": [],
      "source": [
        "def seed_torch(seed = 5101992):\n",
        "    \n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exponentially Decreasing Weights"
      ],
      "metadata": {
        "id": "mq-4N8P1Lfrv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD2LqsOcXYeZ"
      },
      "outputs": [],
      "source": [
        "def exp_decreasing_weights(shifts, alpha):\n",
        "        psi = np.zeros(shifts)\n",
        "        for i in range(shifts):\n",
        "           psi[i] = alpha*(1-alpha)**i\n",
        "        psi = [(j/(np.sum(psi))) for j in psi]  # normalize weights to sum to 1\n",
        "        return psi\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQm-bL29BcjA"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optim, save_path):\n",
        "    \n",
        "    torch.save({'model_state_dict': model.state_dict(), \n",
        "                'optim_state_dict': optim.state_dict()}, \n",
        "               save_path)\n",
        "\n",
        "def load_checkpoint(model, optim, load_path):\n",
        "  \n",
        "    checkpoint = torch.load(load_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optim.load_state_dict(checkpoint['optim_state_dict'])\n",
        "    \n",
        "    return model, optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LS8uNdYuiaA"
      },
      "outputs": [],
      "source": [
        "class StableNBeatsLearner:\n",
        "    \n",
        "    def __init__(self,\n",
        "                 device,\n",
        "                 forecast_length,\n",
        "                 configNBeats):\n",
        "        \n",
        "        gc.collect()\n",
        "                \n",
        "        self.device = device \n",
        "        self.forecast_length = forecast_length\n",
        "        self.configNBeats = configNBeats\n",
        "        \n",
        "        if self.configNBeats[\"loss_function\"] == 1:\n",
        "            self.loss = RMSSE\n",
        "        elif self.configNBeats[\"loss_function\"] == 2:\n",
        "            self.loss = RMSSE_m\n",
        "        elif self.configNBeats[\"loss_function\"] == 3:\n",
        "            self.loss = SMAPE\n",
        "        elif self.configNBeats[\"loss_function\"] == 4:\n",
        "            self.loss = MAPE\n",
        "            \n",
        "        self.rndseed = self.configNBeats[\"rndseed\"]\n",
        "        \n",
        "     \n",
        "        seed_torch(self.rndseed)\n",
        "        \n",
        "        print('--- Model ---')    \n",
        "        self.model = StableNBeatsNet(self.device,\n",
        "                                     self.configNBeats[\"backcast_length_multiplier\"],\n",
        "                                     self.forecast_length,\n",
        "                                     self.configNBeats[\"hidden_layer_units\"],\n",
        "                                     self.configNBeats[\"thetas_dims\"],\n",
        "                                     self.configNBeats[\"share_thetas\"],\n",
        "                                     self.configNBeats[\"nb_blocks_per_stack\"],\n",
        "                                     self.configNBeats[\"n_stacks\"],\n",
        "                                     self.configNBeats[\"share_weights_in_stack\"],                               \n",
        "                                     self.configNBeats[\"dropout\"],\n",
        "                                     self.configNBeats[\"dropout_p\"],\n",
        "                                     self.configNBeats[\"neg_slope\"])\n",
        "        \n",
        "        self.model = self.model.to(self.device)\n",
        "        \n",
        "        self.optim = torch.optim.Adam(self.model.parameters(), \n",
        "                                      lr = self.configNBeats[\"learning_rate\"],\n",
        "                                      weight_decay = self.configNBeats[\"weight_decay\"])\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        self.init_state = copy.deepcopy(self.model.state_dict())\n",
        "        self.init_state_opt = copy.deepcopy(self.optim.state_dict())        \n",
        "        \n",
        "        wandb.watch(self.model)\n",
        "    \n",
        "    \n",
        "    def ts_padding(self, ts_train_data, ts_eval_data):\n",
        "        \n",
        "        # Some time series in the dataset are not long enough to support the specified:\n",
        "        # forecast_length and backcast_length + backcast input shifts\n",
        "        # we use zero padding for the time series that are too short (neutral effect on loss calculations)\n",
        "        # + self.shifts comes from the number of extra observations needed to create the shifted inputs/targets\n",
        "        # + (self.forgins - 1) comes from rolling origin evaluation\n",
        "        \n",
        "        length_train = (self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length + \n",
        "                        self.forecast_length +\n",
        "                        self.shifts)\n",
        "        length_eval = (self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length +\n",
        "                       self.forecast_length +\n",
        "                       self.shifts +\n",
        "                       (self.forigins - 1))\n",
        "        \n",
        "        ts_train_pad = [x if x.size >= length_train else np.pad(x,\n",
        "                                                                (int(length_train - x.size), 0), \n",
        "                                                                'constant', \n",
        "                                                                constant_values = 0) for x in ts_train_data]\n",
        "        ts_eval_pad = [x if x.size >= length_eval else np.pad(x,\n",
        "                                                              (int(length_eval - x.size), 0), \n",
        "                                                              'constant', \n",
        "                                                              constant_values = 0) for x in ts_eval_data]\n",
        "        \n",
        "        return ts_train_pad, ts_eval_pad\n",
        "        \n",
        "        \n",
        "    def make_batch(self, batch_data, shuffle_origin = True):\n",
        "        # If shuffle_origin = True --> batch for training --> random forecast origin based on LH \n",
        "        # If shuffle_origin = False --> batch for evaluation --> fixed forecast origin\n",
        "        \n",
        "        # Split the batch into input_list and target_list\n",
        "        # In x_arr and target_arr: batch x shift x backcats_length/forecast_length\n",
        "        x_arr = np.empty(shape = (len(batch_data), \n",
        "                                  self.shifts + 1, # include the base aswell, not only shifted\n",
        "                                  self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length))\n",
        "        target_arr = np.empty(shape = (len(batch_data), \n",
        "                                       self.shifts + 1, \n",
        "                                       self.forecast_length))\n",
        "        \n",
        "        # For every time series in the batch:\n",
        "        # (1) slice the time series according to specific forecasting origin (depending on shuffle_origin)\n",
        "        # (2) make shifted inputs/targets --> max number of shifts = forecast_length - 1\n",
        "        # (3) fill x_arr and target_arr\n",
        "        for j in range(len(batch_data)):\n",
        "            i = batch_data[j]\n",
        "            \n",
        "            if shuffle_origin: \n",
        "                # suffle_origin --> only in training \n",
        "                \n",
        "                ### --> also pick random scale --> does not result in improved results\n",
        "                ### to remain as close as possible to nbeats paper: do not pick random scale\n",
        "                ### i = i + i * np.random.default_rng().uniform(-0.95, 0.95, 1)\n",
        "                \n",
        "                # pick origin\n",
        "                LH_max_offset = int(self.configNBeats[\"LH\"] * self.forecast_length)\n",
        "                ts_max_offset = int(len(i) -\n",
        "                                    (self.configNBeats[\"backcast_length_multiplier\"] * self.forecast_length + \n",
        "                                     self.forecast_length +\n",
        "                                     self.shifts))\n",
        "                max_offset = min(LH_max_offset, ts_max_offset)\n",
        "                if max_offset < 1:\n",
        "                    offset = np.zeros(1)\n",
        "                else:\n",
        "                    offset = np.random.randint(low = 0, high = max_offset)   \n",
        "            else:\n",
        "                offset = np.zeros(1)\n",
        "            \n",
        "            if offset == 0:\n",
        "                for shift in range(self.shifts + 1):\n",
        "                    if shift == 0:\n",
        "                        x_arr[j, shift, :] = i[-self.forecast_length-self.configNBeats[\"backcast_length_multiplier\"]*self.forecast_length:-self.forecast_length]\n",
        "                        target_arr[j, shift, :] = i[-self.forecast_length:]\n",
        "                    else:\n",
        "                        x_arr[j, shift, :] = i[-self.forecast_length-self.configNBeats[\"backcast_length_multiplier\"]*self.forecast_length-shift:-self.forecast_length-shift]\n",
        "                        target_arr[j, shift, :] = i[-self.forecast_length-shift:-shift]\n",
        "            else:\n",
        "                for shift in range(self.shifts + 1):\n",
        "                    x_arr[j, shift, :] = i[-self.forecast_length-self.configNBeats[\"backcast_length_multiplier\"]*self.forecast_length-offset-shift:-self.forecast_length-offset-shift]\n",
        "                    target_arr[j, shift, :] = i[-self.forecast_length-offset-shift:-offset-shift]\n",
        "                    \n",
        "        return x_arr, target_arr\n",
        "                                        \n",
        "                    \n",
        "    def create_example_plots(self, output, target, actuals_train, final_evaluation = False):\n",
        "        print(\"---example plots creation started---\")\n",
        "        plot_forecasts = torch.cat((actuals_train, output))\n",
        "        plot_actuals = torch.cat((actuals_train, target))\n",
        "        random_sample_forecasts = plot_forecasts.squeeze()\n",
        "        random_sample_actuals = plot_actuals.squeeze()\n",
        "        x_axis = torch.arange(1, random_sample_forecasts.shape[0]+1)\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Scatter(x = x_axis.numpy(), y = random_sample_forecasts.numpy(),\n",
        "                                 mode = 'lines+markers', name = 'forecasts'))\n",
        "        fig.add_trace(go.Scatter(x = x_axis.numpy(), y = random_sample_actuals.numpy(),\n",
        "                                 mode = 'lines+markers', name = 'actuals'))\n",
        "        \n",
        "        # We only visualize examples for last epoch\n",
        "        if not final_evaluation:\n",
        "            wandb.log({\"example_plots_evaluation\": fig})\n",
        "        else:\n",
        "            wandb.log({\"example_plots_final_evaluation\": fig})\n",
        "            \n",
        "   \n",
        "    def evaluate(self, x_arr, target_arr,\n",
        "                 epoch = None,\n",
        "                 need_grad = True,\n",
        "                 early_stop = False):\n",
        "        #print(\"---evaluate---\")\n",
        "        losses = dict()\n",
        "        \n",
        "        # Inputs must be converted to np.array of Tensors (float)\n",
        "        x_arr = torch.from_numpy(x_arr).float().to(self.device)\n",
        "        target_arr = torch.from_numpy(target_arr).float().to(self.device)\n",
        "        \n",
        "        if need_grad:\n",
        "            self.model.train()\n",
        "            self.model.to(self.device)\n",
        "            _, forecast_arr = self.model(x_arr)\n",
        "            \n",
        "            losses_forecast_shifts = 0.0\n",
        "            for shift in range(self.shifts + 1):\n",
        "                losses_forecast_shifts += self.loss(forecast_arr[:, shift, :], \n",
        "                                                    target_arr[:, shift, :], \n",
        "                                                    x_arr[:, shift, :])\n",
        "            losses[\"forecast_accuracy\"] = losses_forecast_shifts / (self.shifts + 1)\n",
        "\n",
        "            #create data structures to contain forecasts for overlapping periods made at (non-)adjacent forecasting origins            \n",
        "            if self.shifts > 0:\n",
        "              #forecasts made at adjacent origins\n",
        "              forecast_shift1_arr_gap0 = torch.zeros((forecast_arr.shape[0], (self.shifts-0)**2),dtype = torch.float).to(self.device)\n",
        "              forecast_shift2_arr_gap0 = torch.zeros((forecast_arr.shape[0], (self.shifts-0)**2),dtype = torch.float).to(self.device)\n",
        "              #forecasts made at origins separated by 2 time periods\n",
        "              forecast_shift1_arr_gap1 = torch.zeros((forecast_arr.shape[0], (self.shifts-1)**2),dtype = torch.float).to(self.device)\n",
        "              forecast_shift2_arr_gap1 = torch.zeros((forecast_arr.shape[0], (self.shifts-1)**2),dtype = torch.float).to(self.device)\n",
        "              #forecasts made at origins separated by 3 time periods\n",
        "              forecast_shift1_arr_gap2 = torch.zeros((forecast_arr.shape[0], (self.shifts-2)**2),dtype = torch.float).to(self.device)\n",
        "              forecast_shift2_arr_gap2 = torch.zeros((forecast_arr.shape[0], (self.shifts-2)**2),dtype = torch.float).to(self.device)\n",
        "              #forecasts made at origins separated by 4 time periods\n",
        "              forecast_shift1_arr_gap3 = torch.zeros((forecast_arr.shape[0], (self.shifts-3)**2),dtype = torch.float).to(self.device)\n",
        "              forecast_shift2_arr_gap3 = torch.zeros((forecast_arr.shape[0], (self.shifts-3)**2),dtype = torch.float).to(self.device)\n",
        "              #forecasts made at origins separated by 5 time periods\n",
        "              forecast_shift1_arr_gap4 = torch.zeros((forecast_arr.shape[0], (self.shifts-4)**2),dtype = torch.float).to(self.device)\n",
        "              forecast_shift2_arr_gap4 = torch.zeros((forecast_arr.shape[0], (self.shifts-4)**2),dtype = torch.float).to(self.device)\n",
        "\n",
        "              #forecast instability components\n",
        "              col = 0\n",
        "              for shift1 in range(0, self.shifts):\n",
        "                shift2 = shift1 + 1\n",
        "                for horizon_m1 in range(self.forecast_length - 1):\n",
        "                  forecast_shift1_arr_gap0[:, col] = forecast_arr[:, shift1, horizon_m1] \n",
        "                  forecast_shift2_arr_gap0[:, col] = forecast_arr[:, shift2, horizon_m1 + 1] \n",
        "                  col += 1\n",
        "              \n",
        "              forecast_instability_gap0 = self.loss(forecast_shift2_arr_gap0[:, 0:5], forecast_shift1_arr_gap0[:, 0:5], x_arr[:, 0, :]) + self.loss(forecast_shift2_arr_gap0[:, 5:10], forecast_shift1_arr_gap0[:, 5:10], x_arr[:, 1, :]) + self.loss(forecast_shift2_arr_gap0[:, 10:15], forecast_shift1_arr_gap0[:, 10:15], x_arr[:, 2, :]) + self.loss(forecast_shift2_arr_gap0[:, 15:20], forecast_shift1_arr_gap0[:, 15:20], x_arr[:, 3, :]) + self.loss(forecast_shift2_arr_gap0[:, 20:25], forecast_shift1_arr_gap0[:, 20:25], x_arr[:, 4, :])\n",
        "              losses[\"forecast_instability_gap0\"] = forecast_instability_gap0 / 5\n",
        "\n",
        "              col = 0\n",
        "              for shift1 in range(0, self.shifts - 1):\n",
        "                shift2 = shift1 + 2\n",
        "                for horizon_m1 in range(self.forecast_length - 2):\n",
        "                    forecast_shift1_arr_gap1[:, col] = forecast_arr[:, shift1, horizon_m1] \n",
        "                    forecast_shift2_arr_gap1[:, col] = forecast_arr[:, shift2, horizon_m1 + 2] \n",
        "                    col += 1\n",
        "              \n",
        "              forecast_instability_gap1 = self.loss(forecast_shift2_arr_gap1[:, 0:4], forecast_shift1_arr_gap1[:, 0:4], x_arr[:, 0, :]) + self.loss(forecast_shift2_arr_gap1[:, 4:8], forecast_shift1_arr_gap1[:, 4:8], x_arr[:, 1, :]) + self.loss(forecast_shift2_arr_gap1[:, 8:12], forecast_shift1_arr_gap1[:, 8:12], x_arr[:, 2, :]) + self.loss(forecast_shift2_arr_gap1[:, 12:16], forecast_shift1_arr_gap1[:, 12:16], x_arr[:, 3, :]) \n",
        "              losses[\"forecast_instability_gap1\"] = forecast_instability_gap1 / 4\n",
        "\n",
        "              col = 0\n",
        "              for shift1 in range(0, self.shifts - 2):\n",
        "                shift2 = shift1 + 3\n",
        "                for horizon_m1 in range(self.forecast_length - 3):\n",
        "                    forecast_shift1_arr_gap2[:, col] = forecast_arr[:, shift1, horizon_m1] \n",
        "                    forecast_shift2_arr_gap2[:, col] = forecast_arr[:, shift2, horizon_m1 + 3] \n",
        "                    col += 1\n",
        "\n",
        "              forecast_instability_gap2 = self.loss(forecast_shift2_arr_gap2[:, 0:3], forecast_shift1_arr_gap2[:, 0:3], x_arr[:, 0, :]) + self.loss(forecast_shift2_arr_gap2[:, 3:6], forecast_shift1_arr_gap2[:, 3:6], x_arr[:, 1, :]) + self.loss(forecast_shift2_arr_gap2[:, 6:9], forecast_shift1_arr_gap2[:, 6:9], x_arr[:, 2, :]) \n",
        "              losses[\"forecast_instability_gap2\"] = forecast_instability_gap2 / 3\n",
        "\n",
        "              col = 0\n",
        "              for shift1 in range(0, self.shifts - 3):\n",
        "                shift2 = shift1 + 4\n",
        "                for horizon_m1 in range(self.forecast_length - 4):\n",
        "                    forecast_shift1_arr_gap3[:, col] = forecast_arr[:, shift1, horizon_m1] \n",
        "                    forecast_shift2_arr_gap3[:, col] = forecast_arr[:, shift2, horizon_m1 + 4] \n",
        "                    col += 1\n",
        "\n",
        "              forecast_instability_gap3 = self.loss(forecast_shift2_arr_gap3[:, 0:2], forecast_shift1_arr_gap3[:, 0:2], x_arr[:, 0, :]) + self.loss(forecast_shift2_arr_gap3[:, 2:4], forecast_shift1_arr_gap3[:, 2:4], x_arr[:, 1, :])\n",
        "              losses[\"forecast_instability_gap3\"] = forecast_instability_gap3 / 2\n",
        "\n",
        "              col = 0\n",
        "              for shift1 in range(0, self.shifts - 4):\n",
        "                shift2 = shift1 + 5\n",
        "                for horizon_m1 in range(self.forecast_length - 5):\n",
        "                    forecast_shift1_arr_gap4[:, col] = forecast_arr[:, shift1, horizon_m1] \n",
        "                    forecast_shift2_arr_gap4[:, col] = forecast_arr[:, shift2, horizon_m1 + 5] \n",
        "                    col += 1\n",
        "\n",
        "              losses[\"forecast_instability_gap4\"] = self.loss(forecast_shift2_arr_gap4,\n",
        "                                                   forecast_shift1_arr_gap4,\n",
        "                                                    x_arr[:, 0, :])\n",
        "              \n",
        "            else: \n",
        "              losses[\"forecast_instability_gap0\"]=torch.zeros(1)\n",
        "              losses[\"forecast_instability_gap1\"]=torch.zeros(1)\n",
        "              losses[\"forecast_instability_gap2\"]=torch.zeros(1)\n",
        "              losses[\"forecast_instability_gap3\"]=torch.zeros(1)\n",
        "              losses[\"forecast_instability_gap4\"]=torch.zeros(1)                  \n",
        "                \n",
        "        else: \n",
        "            with torch.no_grad():\n",
        "                self.model.eval()\n",
        "                self.model.to(self.device)\n",
        "                _, forecast_arr = self.model(x_arr)\n",
        "                \n",
        "                losses_forecast_shifts = 0.0\n",
        "                for shift in range(self.shifts + 1):\n",
        "                    losses_forecast_shifts += self.loss(forecast_arr[:, shift, :], target_arr[:, shift, :], x_arr[:, shift, :])\n",
        "                losses[\"forecast_accuracy\"] = losses_forecast_shifts / (self.shifts + 1) #plus 1 want origin ook meetellen \n",
        "                            \n",
        "            if self.shifts > 0:\n",
        "              #forecasts made at adjacent origins\n",
        "              forecast_shift1_arr_gap0 = torch.zeros((forecast_arr.shape[0], (self.shifts-0)**2),dtype = torch.float).to(self.device)\n",
        "              forecast_shift2_arr_gap0 = torch.zeros((forecast_arr.shape[0], (self.shifts-0)**2),dtype = torch.float).to(self.device)\n",
        "              #forecasts made at origins separated by 2 time periods\n",
        "              forecast_shift1_arr_gap1 = torch.zeros((forecast_arr.shape[0], (self.shifts-1)**2),dtype = torch.float).to(self.device)\n",
        "              forecast_shift2_arr_gap1 = torch.zeros((forecast_arr.shape[0], (self.shifts-1)**2),dtype = torch.float).to(self.device)\n",
        "              #forecasts made at origins separated by 3 time periods\n",
        "              forecast_shift1_arr_gap2 = torch.zeros((forecast_arr.shape[0], (self.shifts-2)**2),dtype = torch.float).to(self.device)\n",
        "              forecast_shift2_arr_gap2 = torch.zeros((forecast_arr.shape[0], (self.shifts-2)**2),dtype = torch.float).to(self.device)\n",
        "              #forecasts made at origins separated by 4 time periods\n",
        "              forecast_shift1_arr_gap3 = torch.zeros((forecast_arr.shape[0], (self.shifts-3)**2),dtype = torch.float).to(self.device)\n",
        "              forecast_shift2_arr_gap3 = torch.zeros((forecast_arr.shape[0], (self.shifts-3)**2),dtype = torch.float).to(self.device)\n",
        "              #forecasts made at origins separated by 5 time periods \n",
        "              forecast_shift1_arr_gap4 = torch.zeros((forecast_arr.shape[0], (self.shifts-4)**2),dtype = torch.float).to(self.device)\n",
        "              forecast_shift2_arr_gap4 = torch.zeros((forecast_arr.shape[0], (self.shifts-4)**2),dtype = torch.float).to(self.device)\n",
        "\n",
        "              #forecast instability components\n",
        "              col = 0\n",
        "              for shift1 in range(0, self.shifts):\n",
        "                shift2 = shift1 + 1\n",
        "                for horizon_m1 in range(self.forecast_length - 1):\n",
        "                  forecast_shift1_arr_gap0[:, col] = forecast_arr[:, shift1, horizon_m1] \n",
        "                  forecast_shift2_arr_gap0[:, col] = forecast_arr[:, shift2, horizon_m1 + 1] \n",
        "                  col += 1\n",
        "              \n",
        "              forecast_instability_gap0 = self.loss(forecast_shift2_arr_gap0[:, 0:5], forecast_shift1_arr_gap0[:, 0:5], x_arr[:, 0, :]) + self.loss(forecast_shift2_arr_gap0[:, 5:10], forecast_shift1_arr_gap0[:, 5:10], x_arr[:, 1, :]) + self.loss(forecast_shift2_arr_gap0[:, 10:15], forecast_shift1_arr_gap0[:, 10:15], x_arr[:, 2, :]) + self.loss(forecast_shift2_arr_gap0[:, 15:20], forecast_shift1_arr_gap0[:, 15:20], x_arr[:, 3, :]) + self.loss(forecast_shift2_arr_gap0[:, 20:25], forecast_shift1_arr_gap0[:, 20:25], x_arr[:, 4, :])\n",
        "              losses[\"forecast_instability_gap0\"] = forecast_instability_gap0 / 5\n",
        "\n",
        "              col = 0\n",
        "              for shift1 in range(0, self.shifts - 1):\n",
        "                shift2 = shift1 + 2\n",
        "                for horizon_m1 in range(self.forecast_length - 2):\n",
        "                    forecast_shift1_arr_gap1[:, col] = forecast_arr[:, shift1, horizon_m1] \n",
        "                    forecast_shift2_arr_gap1[:, col] = forecast_arr[:, shift2, horizon_m1 + 2] \n",
        "                    col += 1\n",
        "              \n",
        "              forecast_instability_gap1 = self.loss(forecast_shift2_arr_gap1[:, 0:4], forecast_shift1_arr_gap1[:, 0:4], x_arr[:, 0, :]) + self.loss(forecast_shift2_arr_gap1[:, 4:8], forecast_shift1_arr_gap1[:, 4:8], x_arr[:, 1, :]) + self.loss(forecast_shift2_arr_gap1[:, 8:12], forecast_shift1_arr_gap1[:, 8:12], x_arr[:, 2, :]) + self.loss(forecast_shift2_arr_gap1[:, 12:16], forecast_shift1_arr_gap1[:, 12:16], x_arr[:, 3, :]) \n",
        "              losses[\"forecast_instability_gap1\"] = forecast_instability_gap1 / 4\n",
        "              \n",
        "\n",
        "              col = 0\n",
        "              for shift1 in range(0, self.shifts - 2):\n",
        "                shift2 = shift1 + 3\n",
        "                for horizon_m1 in range(self.forecast_length - 3):\n",
        "                    forecast_shift1_arr_gap2[:, col] = forecast_arr[:, shift1, horizon_m1] \n",
        "                    forecast_shift2_arr_gap2[:, col] = forecast_arr[:, shift2, horizon_m1 + 3] \n",
        "                    col += 1\n",
        "\n",
        "              forecast_instability_gap2 = self.loss(forecast_shift2_arr_gap2[:, 0:3], forecast_shift1_arr_gap2[:, 0:3], x_arr[:, 0, :]) + self.loss(forecast_shift2_arr_gap2[:, 3:6], forecast_shift1_arr_gap2[:, 3:6], x_arr[:, 1, :]) + self.loss(forecast_shift2_arr_gap2[:, 6:9], forecast_shift1_arr_gap2[:, 6:9], x_arr[:, 2, :])\n",
        "              losses[\"forecast_instability_gap2\"]= forecast_instability_gap2 / 3\n",
        "\n",
        "\n",
        "              col = 0\n",
        "              for shift1 in range(0, self.shifts - 3):\n",
        "                shift2 = shift1 + 4\n",
        "                for horizon_m1 in range(self.forecast_length - 4):\n",
        "                    forecast_shift1_arr_gap3[:, col] = forecast_arr[:, shift1, horizon_m1] \n",
        "                    forecast_shift2_arr_gap3[:, col] = forecast_arr[:, shift2, horizon_m1 + 4] \n",
        "                    col += 1\n",
        "\n",
        "              forecast_instability_gap3 = self.loss(forecast_shift2_arr_gap3[:, 0:2], forecast_shift1_arr_gap3[:, 0:2], x_arr[:, 0, :]) + self.loss(forecast_shift2_arr_gap3[:, 2:4], forecast_shift1_arr_gap3[:, 2:4], x_arr[:, 1, :]) \n",
        "              losses[\"forecast_instability_gap3\"] = forecast_instability_gap3 / 2\n",
        "\n",
        "              col = 0\n",
        "              for shift1 in range(0, self.shifts - 4):\n",
        "                shift2 = shift1 + 5\n",
        "                for horizon_m1 in range(self.forecast_length - 5):\n",
        "                    forecast_shift1_arr_gap4[:, col] = forecast_arr[:, shift1, horizon_m1] \n",
        "                    forecast_shift2_arr_gap4[:, col] = forecast_arr[:, shift2, horizon_m1 + 5] \n",
        "                    col += 1\n",
        "\n",
        "              losses[\"forecast_instability_gap4\"] = self.loss(forecast_shift2_arr_gap4,\n",
        "                                                    forecast_shift1_arr_gap4,\n",
        "                                                    x_arr[:, 0, :])\n",
        "              \n",
        "            else: \n",
        "              losses[\"forecast_instability_gap0\"]=torch.zeros(1)\n",
        "              losses[\"forecast_instability_gap1\"]=torch.zeros(1)\n",
        "              losses[\"forecast_instability_gap2\"]=torch.zeros(1)\n",
        "              losses[\"forecast_instability_gap3\"]=torch.zeros(1)\n",
        "              losses[\"forecast_instability_gap4\"]=torch.zeros(1) \n",
        "                    \n",
        "            if not self.disable_plot:\n",
        "                    if early_stop:\n",
        "                        # Plot validation examples - of standard/unshifted input - for last epoch before break\n",
        "                        # This part of the evaluation function is only called after training has been forced to stop\n",
        "                        self.create_example_plots(forecast_arr[0, 0, :], target_arr[0, 0, :], x_arr[0, 0, :])\n",
        "                    else:\n",
        "                        # Plot validation examples - of standard/unshifted input - for last epoch\n",
        "                        # This part of the evaluation function is called after training has been completed\n",
        "                        if (epoch == self.configNBeats[\"epochs\"]):\n",
        "                          self.create_example_plots(forecast_arr[0, 0, :], target_arr[0, 0, :], x_arr[0, 0, :])\n",
        "        \n",
        "        return losses\n",
        "    \n",
        "    # Training of net (training data can include validation data) + validation or testing\n",
        "    def train_net(self,\n",
        "                  ts_train_m4m,\n",
        "                  ts_eval_m4m,\n",
        "                  forigins,\n",
        "                  validation = True,\n",
        "                  validation_earlystop = False,\n",
        "                  disable_plot = True):\n",
        "        \n",
        "        self.forigins = forigins\n",
        "        self.shifts = self.configNBeats[\"shifts\"]\n",
        "        self.validation = validation\n",
        "        self.validation_earlystop = validation_earlystop\n",
        "        self.disable_plot = disable_plot\n",
        "        #assert self.shifts < self.forecast_length # max allowed number of shifts is forecast_length - 1; anders geen overlap meer, dit komt overeen met onze bovengrens n=5 aangezien we h = 6 gebruiken\n",
        "\n",
        "        # Data preprocessing depends on backcast_length_multiplier\n",
        "        ts_train_pad, ts_eval_pad = self.ts_padding(ts_train_m4m, ts_eval_m4m)\n",
        "        ts_train_pad = np.array(ts_train_pad, dtype = object)\n",
        "        ts_eval_pad = np.array(ts_eval_pad, dtype = object)\n",
        "        \n",
        "        print('--- Training ---')\n",
        "        # Containers to save train/evaluation losses and parameters\n",
        "        tloss_combined, tloss_forecast_accuracy, tloss_forecast_stability = [], [], []\n",
        "        eloss_combined, eloss_forecast_accuracy, eloss_forecast_stability = [], [], []\n",
        "        #params = []\n",
        "        \n",
        "        # Main training loop\n",
        "        self.model.load_state_dict(self.init_state)\n",
        "        self.optim.load_state_dict(self.init_state_opt)\n",
        "            \n",
        "        seed_torch(self.rndseed)\n",
        "        # Initialize early stopping object\n",
        "        if self.validation_earlystop:\n",
        "            early_stopping = EarlyStopping(patience = self.configNBeats[\"patience\"], verbose = True)\n",
        "\n",
        "        # Weights used for the instability components\n",
        "        psi_list = exp_decreasing_weights(shifts=self.shifts, alpha=self.configNBeats[\"alpha\"]) # used for N-BEATS-MPS with exponentially decreasing weights\n",
        "        #psi_list = [0.2,0.2,0.2,0.2,0.2] # used for N-BEATS-MPS with constant weights\n",
        "\n",
        "        for epoch in range(1, self.configNBeats[\"epochs\"]+1):\n",
        "            \n",
        "            start_time = time()\n",
        "            # Shuffle train data\n",
        "            np.random.shuffle(ts_train_pad)\n",
        "            # Determine number of batches per epoch\n",
        "            num_batches = int(ts_train_pad.shape[0] / self.configNBeats[\"batch_size\"])\n",
        "            \n",
        "            # Training per epoch\n",
        "            avg_tloss_combined_epoch = 0.0\n",
        "            avg_tloss_forecast_accuracy_epoch = 0.0\n",
        "            avg_tloss_forecast_stability_epoch = 0.0\n",
        "            \n",
        "            for k in range(num_batches):\n",
        "                \n",
        "                batch = np.array(ts_train_pad[k*self.configNBeats[\"batch_size\"]:(k+1)*self.configNBeats[\"batch_size\"]])\n",
        "                x_arr, target_arr = self.make_batch(batch, shuffle_origin = True)\n",
        "                \n",
        "                self.optim.zero_grad()\n",
        "                losses_batch = self.evaluate(x_arr, target_arr,\n",
        "                                             epoch, need_grad = True, \n",
        "                                             early_stop = False)\n",
        "              \n",
        "                if self.shifts > 0: \n",
        "                    loss_combined = ((self.configNBeats[\"lambda\"] * (psi_list[0] * losses_batch[\"forecast_instability_gap0\"] + psi_list[1] * losses_batch[\"forecast_instability_gap1\"] + psi_list[2] * losses_batch[\"forecast_instability_gap2\"] + psi_list[3] * losses_batch[\"forecast_instability_gap3\"] + psi_list[4] * losses_batch[\"forecast_instability_gap4\"])) +\n",
        "                                     ((1 - self.configNBeats[\"lambda\"]) * losses_batch[\"forecast_accuracy\"]))\n",
        "                else:\n",
        "                    loss_combined = losses_batch[\"forecast_accuracy\"]\n",
        "                #loss_combined.backward()\n",
        "                #nn.utils.clip_grad_value_(self.model.parameters(), )\n",
        "                #self.optim.step()\n",
        "\n",
        "\n",
        "                # Avoidance of nan-values \n",
        "                # Save checkpoint before weight updates\n",
        "                save_checkpoint(self.model, self.optim, \n",
        "                                '/content/drive/MyDrive/checkpoint.pt')\n",
        "            \n",
        "                loss_combined.backward()\n",
        "                #torch.autograd.detect_anomaly(check_nan = True)\n",
        "                #nn.utils.clip_grad_norm_(self.model.parameters(), .1)\n",
        "                self.optim.step()\n",
        "\n",
        "                # Load checkpoint if nan values are produced\n",
        "                losses_batch_checkpoint = self.evaluate(x_arr, target_arr,\n",
        "                                                        epoch, need_grad = True, \n",
        "                                                        early_stop = False)\n",
        "                if torch.isnan(losses_batch_checkpoint[\"forecast_accuracy\"]):\n",
        "                    self.model, self.optim = load_checkpoint(self.model, self.optim,\n",
        "                                                                 '/content/drive/MyDrive/checkpoint.pt')\n",
        "                    print('Skip iteration')\n",
        "                \n",
        "                \n",
        "\n",
        "                \n",
        "                #params = self.model.parameters()\n",
        "                #total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "                #if (epoch == 1 or epoch == self.configNBeats[\"epochs\"]) and k == 0:\n",
        "                #    print('Epoch {}/{} \\t n_learnable_pars={:.4f}'.format(\n",
        "                #        epoch,\n",
        "                #        self.configNBeats[\"epochs\"],\n",
        "                #        total_params))\n",
        "                \n",
        "                wandb.log({\"tloss_comb_step\": loss_combined,\n",
        "                           \"tloss_fcacc_step\": losses_batch[\"forecast_accuracy\"],\n",
        "                           \"tloss_fcstab_step\": psi_list[0] * losses_batch[\"forecast_instability_gap0\"] + psi_list[1] * losses_batch[\"forecast_instability_gap1\"] + psi_list[2] * losses_batch[\"forecast_instability_gap2\"] + psi_list[3] * losses_batch[\"forecast_instability_gap3\"] + psi_list[4] * losses_batch[\"forecast_instability_gap4\"]}) \n",
        "                avg_tloss_combined_epoch += (loss_combined / num_batches)\n",
        "                avg_tloss_forecast_accuracy_epoch += (losses_batch[\"forecast_accuracy\"] / num_batches)\n",
        "                avg_tloss_forecast_stability_epoch += ((psi_list[0] * losses_batch[\"forecast_instability_gap0\"] + psi_list[1] * losses_batch[\"forecast_instability_gap1\"] + psi_list[2] * losses_batch[\"forecast_instability_gap2\"] + psi_list[3] * losses_batch[\"forecast_instability_gap3\"] + psi_list[4] * losses_batch[\"forecast_instability_gap4\"]) / num_batches)\n",
        "                \n",
        "            if self.validation: # validation_full and validation_earlystop\n",
        "                \n",
        "                # Evaluation per epoch\n",
        "                avg_eloss_combined_epoch = 0.0\n",
        "                avg_eloss_forecast_accuracy_epoch = 0.0\n",
        "                avg_eloss_forecast_stability_epoch = 0.0\n",
        "\n",
        "                for forigin in range(self.forigins):\n",
        "\n",
        "                    # Only one batch, but one batch per forecast origin\n",
        "                    if forigin < self.forigins-1:\n",
        "                        eval_data_subset = np.array([x[:(-18 + forigin + self.forecast_length)] for x in ts_eval_pad],\n",
        "                                                   dtype = object)\n",
        "                    else:\n",
        "                        eval_data_subset = np.array([x for x in ts_eval_pad], dtype = object)\n",
        "                    x_arr, target_arr = self.make_batch(eval_data_subset, shuffle_origin = False)\n",
        "                    \n",
        "                    losses_evaluation = self.evaluate(x_arr, target_arr,\n",
        "                                                      epoch, need_grad = False,\n",
        "                                                      early_stop = False)\n",
        "                    if self.shifts > 0:\n",
        "                      loss_combined = ((self.configNBeats[\"lambda\"] * (psi_list[0] * losses_evaluation[\"forecast_instability_gap0\"] + psi_list[1] * losses_evaluation[\"forecast_instability_gap1\"] + psi_list[2] * losses_evaluation[\"forecast_instability_gap2\"] + psi_list[3] * losses_evaluation[\"forecast_instability_gap3\"] + psi_list[4] * losses_evaluation[\"forecast_instability_gap4\"])) +\n",
        "                                     ((1 - self.configNBeats[\"lambda\"]) * losses_evaluation[\"forecast_accuracy\"]))\n",
        "\n",
        "                    else:\n",
        "                      loss_combined = losses_evaluation[\"forecast_accuracy\"]\n",
        "\n",
        "                    avg_eloss_combined_epoch += (loss_combined / self.forigins)\n",
        "                    avg_eloss_forecast_accuracy_epoch += (losses_evaluation[\"forecast_accuracy\"] / self.forigins)\n",
        "                    avg_eloss_forecast_stability_epoch += ((psi_list[0] * losses_evaluation[\"forecast_instability_gap0\"] + psi_list[1] * losses_evaluation[\"forecast_instability_gap1\"] + psi_list[2] * losses_evaluation[\"forecast_instability_gap2\"] + psi_list[3] * losses_evaluation[\"forecast_instability_gap3\"] + psi_list[4] * losses_evaluation[\"forecast_instability_gap4\"]) / self.forigins)\n",
        "                \n",
        "                elapsed_time = time() - start_time\n",
        "\n",
        "                print('Epoch {}/{} \\t tloss_combined={:.4f} \\t eloss_combined={:.4f} \\t time={:.2f}s'.format(\n",
        "                    epoch,\n",
        "                    self.configNBeats[\"epochs\"],\n",
        "                    avg_tloss_combined_epoch,\n",
        "                    avg_eloss_combined_epoch,\n",
        "                    elapsed_time))\n",
        "                \n",
        "                wandb.log({\"epoch\": epoch,\n",
        "                           \"tloss_comb_evol\": avg_tloss_combined_epoch,\n",
        "                           \"tloss_fcacc_evol\": avg_tloss_forecast_accuracy_epoch,\n",
        "                           \"tloss_fcstab_evol\": avg_tloss_forecast_stability_epoch,\n",
        "                           \"eloss_comb_evol\": avg_eloss_combined_epoch,\n",
        "                           \"eloss_fcacc_evol\": avg_eloss_forecast_accuracy_epoch,\n",
        "                           \"eloss_fcstab_evol\": avg_eloss_forecast_stability_epoch})\n",
        "                \n",
        "                if self.validation_earlystop: # validation_earlystop \n",
        "\n",
        "                    # early_stopping needs the average epoch validation loss to check if it has decreased, \n",
        "                    # and if it has, it will make a checkpoint of the current model\n",
        "                    early_stopping(avg_eloss_combined_epoch, self.model)\n",
        "\n",
        "                    if early_stopping.early_stop:\n",
        "                        print(\"Early stopping\")\n",
        "                        # Load the last checkpoint with the best model\n",
        "                        self.model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "                        # Produce plots for final epoch before break\n",
        "                        if not self.disable_plot:\n",
        "                            for forigin in range(self.forigins):\n",
        "                                # Only one batch, but one batch per forecast origin\n",
        "                                if forigin < self.forigins-1:\n",
        "                                    eval_data_subset = np.array([x[:(-18 + forigin + self.forecast_length)] for x in ts_eval_pad],\n",
        "                                                                dtype = object)\n",
        "                                else:\n",
        "                                    eval_data_subset = np.array([x for x in ts_eval_pad], dtype = object)\n",
        "                                x_arr, target_arr = self.make_batch(eval_data_subset, shuffle_origin = False)\n",
        "\n",
        "                                losses_evaluation = self.evaluate(x_arr, target_arr,\n",
        "                                                                  epoch, need_grad = False,\n",
        "                                                                  early_stop = True)\n",
        "                        # Break loop over epochs\n",
        "                        break\n",
        "                    \n",
        "            else: # testing\n",
        "                \n",
        "                elapsed_time = time() - start_time\n",
        "\n",
        "                print('Epoch {}/{} \\t tloss_combined={:.4f} \\t time={:.2f}s'.format(\n",
        "                    epoch,\n",
        "                    self.configNBeats[\"epochs\"],\n",
        "                    avg_tloss_combined_epoch,\n",
        "                    elapsed_time))\n",
        "                \n",
        "                wandb.log({\"epoch\": epoch,\n",
        "                           \"tloss_comb_evol\": avg_tloss_combined_epoch,\n",
        "                           \"tloss_fcacc_evol\": avg_tloss_forecast_accuracy_epoch,\n",
        "                           \"tloss_fcstab_evol\": avg_tloss_forecast_stability_epoch})\n",
        "\n",
        "        wandb.log({\"tloss_comb\": avg_tloss_combined_epoch,\n",
        "                   \"tloss_fcacc\": avg_tloss_forecast_accuracy_epoch,\n",
        "                   \"tloss_fcstab\": avg_tloss_forecast_stability_epoch})\n",
        "        \n",
        "        print('--- Training done ---')\n",
        "        print('--- Final evaluation ---')\n",
        "        \n",
        "        print('--- M4 evaluation ---')\n",
        "        \n",
        "        # Containers to save actuals and forecasts\n",
        "        actuals = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
        "        forecasts = np.empty(shape = (len(ts_eval_pad), self.forigins, self.forecast_length)) # n_series, forigin, forecast_length\n",
        "        \n",
        "        # Forecasts for each origin in rolling_window\n",
        "        for forigin in range(self.forigins):\n",
        "            \n",
        "            # Only one batch, but one batch per forecast origin\n",
        "            if forigin < self.forigins-1:\n",
        "                eval_data_subset = np.array([x[:(-18 + forigin + self.forecast_length)] for x in ts_eval_pad],\n",
        "                                           dtype = object)\n",
        "            else:\n",
        "                eval_data_subset = np.array([x for x in ts_eval_pad], dtype = object)\n",
        "            x_arr, target_arr = self.make_batch(eval_data_subset, shuffle_origin = False)\n",
        "            \n",
        "            # Produce forecasts for subset of test data\n",
        "            x_arr = torch.from_numpy(x_arr).float().to(self.device)\n",
        "            target_arr = torch.from_numpy(target_arr).float().to(self.device)\n",
        "            with torch.no_grad():\n",
        "                self.model.eval()\n",
        "                self.model.to(self.device)\n",
        "                _, forecast_arr = self.model(x_arr)\n",
        "                \n",
        "            x_arr = x_arr.cpu() \n",
        "            target_arr = target_arr.cpu()\n",
        "            forecast_arr = forecast_arr.cpu()\n",
        "                \n",
        "            # Plot 10 random examples per origin - of standard/unshifted input\n",
        "            sample_ids = np.random.randint(low = 0, high = int(x_arr.shape[0]), size = 10)\n",
        "            for sample_id in sample_ids:\n",
        "                self.create_example_plots(forecast_arr[sample_id, 0, :], \n",
        "                                          target_arr[sample_id, 0, :], \n",
        "                                          x_arr[sample_id, 0, :],\n",
        "                                          final_evaluation = True)\n",
        "                \n",
        "            # Save to containers\n",
        "            forecasts[:, forigin, :] = forecast_arr[:, 0, :]\n",
        "            actuals[:, forigin, :] = target_arr[:, 0, :]\n",
        "\n",
        "        \n",
        "        # Compute accuracy sMAPE\n",
        "        sMAPE = 200 * np.mean(np.abs(actuals - forecasts) / (np.abs(forecasts) + np.abs(actuals)))\n",
        "\n",
        "        #Forecast helper \n",
        "        \n",
        "        forecasts_helper = np.full((actuals.shape[0], \n",
        "                                    self.forigins,\n",
        "                                    (self.forecast_length - 1) + self.forigins), np.nan)\n",
        "        \n",
        "        # n_series x self.forigins x ((forecast_length - 1) + forigins)\n",
        "        for forigin in range(self.forigins):\n",
        "            forecasts_helper[:, forigin, forigin:(forigin + self.forecast_length)] = forecasts[:, forigin, :]\n",
        "\n",
        "        # Compute sMAPC for different gaps in time periods between forecasting origins\n",
        "\n",
        "         # GAP = 1\n",
        "\n",
        "        absolute_changes_nom1 = np.abs(np.diff(forecasts_helper, axis = 1))\n",
        "        # n_series x (self.forigins - 1) x ((forecast_length - 1) + forigins)\n",
        "        absolute_changes_nom1clean = np.delete(absolute_changes_nom1, [0, (self.forecast_length - 1) + self.forigins - 1], 2)\n",
        "        # n_series x (self.forigins - 1) x ((forecast_length - 1) + forigins - 2)\n",
        "        absolute_changes_denom1 = np.abs(forecasts_helper[:,:-1,:]) + np.abs(forecasts_helper[:,1:,:])\n",
        "        # n_series x (self.forigins - 1) x ((forecast_length - 1) + forigins)\n",
        "        absolute_changes_denom1clean = np.delete(absolute_changes_denom1, [0, (self.forecast_length - 1) + self.forigins - 1], 2)\n",
        "        # n_series x (self.forigins - 1) x ((forecast_length - 1) + forigins - 2)\n",
        "        sMAPC1 = 200 * np.nanmean(absolute_changes_nom1clean/absolute_changes_denom1clean)\n",
        "\n",
        "        # GAP = 2\n",
        "\n",
        "\n",
        "        absolute_changes_nom2 = np.abs(forecasts_helper[:,:-2,:] - forecasts_helper[:,2:,:])  \n",
        "        absolute_changes_nom2clean = np.delete(absolute_changes_nom2, [0, 1, (self.forecast_length + self.forigins -3), (self.forecast_length + self.forigins - 2)], 2)\n",
        "\n",
        "        absolute_changes_denom2 = np.abs(forecasts_helper[:,:-2,:]) + np.abs(forecasts_helper[:,2:,:])\n",
        "        absolute_changes_denom2clean = np.delete(absolute_changes_denom2, [0, 1, (self.forecast_length + self.forigins -3), (self.forecast_length + self.forigins - 2)], 2)\n",
        "\n",
        "        sMAPC2 = 200 * np.nanmean(absolute_changes_nom2clean/absolute_changes_denom2clean)\n",
        "\n",
        "        # GAP = 3\n",
        "\n",
        "        absolute_changes_nom3 = np.abs(forecasts_helper[:,:-3,:] - forecasts_helper[:,3:,:])\n",
        "        absolute_changes_nom3clean = np.delete(absolute_changes_nom3, [0, 1, 2, (self.forecast_length + self.forigins -4), (self.forecast_length + self.forigins -3), (self.forecast_length + self.forigins - 2)], 2)\n",
        "\n",
        "        absolute_changes_denom3 = np.abs(forecasts_helper[:,:-3,:]) + np.abs(forecasts_helper[:,3:,:])\n",
        "        absolute_changes_denom3clean = np.delete(absolute_changes_denom3, [0, 1, 2, (self.forecast_length + self.forigins -4), (self.forecast_length + self.forigins -3), (self.forecast_length + self.forigins - 2)], 2)\n",
        "\n",
        "        sMAPC3 = 200 * np.nanmean(absolute_changes_nom3clean/absolute_changes_denom3clean)\n",
        "\n",
        "        # GAP = 4\n",
        "\n",
        "        absolute_changes_nom4 = np.abs(forecasts_helper[:,:-4,:] - forecasts_helper[:,4:,:])\n",
        "        absolute_changes_nom4clean = np.delete(absolute_changes_nom4, [0, 1, 2, 3, (self.forecast_length + self.forigins -5), (self.forecast_length + self.forigins -4), (self.forecast_length + self.forigins -3), (self.forecast_length + self.forigins - 2)], 2)\n",
        "\n",
        "        absolute_changes_denom4 = np.abs(forecasts_helper[:,:-4,:]) + np.abs(forecasts_helper[:,4:,:])\n",
        "        absolute_changes_denom4clean = np.delete(absolute_changes_denom4, [0, 1, 2, 3, (self.forecast_length + self.forigins -5),  (self.forecast_length + self.forigins -4), (self.forecast_length + self.forigins -3), (self.forecast_length + self.forigins - 2)], 2)\n",
        "\n",
        "        sMAPC4 = 200 * np.nanmean(absolute_changes_nom4clean/absolute_changes_denom4clean)\n",
        "\n",
        "        # GAP = 5\n",
        "\n",
        "        absolute_changes_nom5 = np.abs(forecasts_helper[:,:-5,:] - forecasts_helper[:,5:,:])\n",
        "        absolute_changes_nom5clean = np.delete(absolute_changes_nom5, [0, 1, 2, 3, 4, (self.forecast_length + self.forigins -6), (self.forecast_length + self.forigins -5), (self.forecast_length + self.forigins -4), (self.forecast_length + self.forigins -3), (self.forecast_length + self.forigins - 2)], 2)\n",
        "\n",
        "        absolute_changes_denom5 = np.abs(forecasts_helper[:,:-5,:]) + np.abs(forecasts_helper[:,5:,:])\n",
        "        absolute_changes_denom5clean = np.delete(absolute_changes_denom5, [0, 1, 2, 3, 4, (self.forecast_length + self.forigins -6), (self.forecast_length + self.forigins -5), (self.forecast_length + self.forigins -4), (self.forecast_length + self.forigins -3), (self.forecast_length + self.forigins - 2)], 2)\n",
        "\n",
        "        sMAPC5 = 200 * np.nanmean(absolute_changes_nom5clean/absolute_changes_denom5clean)\n",
        "\n",
        "        #Average sMPAC\n",
        "\n",
        "        sMAPCAvg = (sMAPC1 + sMAPC2 + sMAPC3 + sMAPC4 + sMAPC5)*0.2\n",
        "        \n",
        "        print('sMAPE_m4m={:.4f} \\t sMAPC1_m4m = {:.4f} \\t sMAPC2_m4m = {:.4f} \\t sMAPC3_m4m = {:.4f} \\t sMAPC4_m4m = {:.4f}\\t sMAPC5_m4m = {:.4f} \\t sMAPCAvg_m4m = {:.4f}'.format(sMAPE, sMAPC1, sMAPC2, sMAPC3, sMAPC4, sMAPC5, sMAPCAvg))\n",
        "        \n",
        "        wandb.log({\"sMAPE_m4m\": sMAPE,\n",
        "                   \"sMAPC1_m4m\": sMAPC1,\n",
        "                   \"sMAPC2_m4m\": sMAPC2,\n",
        "                   \"sMAPC3_m4m\": sMAPC3,\n",
        "                   \"sMAPC4_m4m\": sMAPC4,\n",
        "                   \"sMAPC5_m4m\": sMAPC5,\n",
        "                   \"sMAPCAvg_m4m\": sMAPCAvg})\n",
        "        \n",
        "        # n_series, forigin, forecast_length\n",
        "        fc_colnames = [str(i) for i in range(1, self.forecast_length + 1)]\n",
        "        \n",
        "        actuals_np = actuals#.numpy()\n",
        "        m,n,r = actuals_np.shape\n",
        "        actuals_arr = np.column_stack((np.repeat(np.arange(m) + 1, n), \n",
        "                                       np.tile(np.arange(n) + 1, m),\n",
        "                                       actuals_np.reshape(m*n, -1)))\n",
        "        actuals_df = pd.DataFrame(actuals_arr, columns = ['item_id', 'fc_origin'] + fc_colnames)\n",
        "        helper_col = ['actual'] * len(actuals_df)\n",
        "        actuals_df['type'] = helper_col\n",
        "        \n",
        "        forecasts_np = forecasts#.numpy()\n",
        "        m,n,r = forecasts_np.shape\n",
        "        forecasts_arr = np.column_stack((np.repeat(np.arange(m) + 1, n), \n",
        "                                         np.tile(np.arange(n) + 1, m),\n",
        "                                         forecasts_np.reshape(m*n, -1)))\n",
        "        forecasts_df = pd.DataFrame(forecasts_arr, columns = ['item_id', 'fc_origin'] + fc_colnames)\n",
        "        helper_col = ['forecast'] * len(forecasts_df)\n",
        "        forecasts_df['type'] = helper_col\n",
        "        \n",
        "        output_df_m4m = pd.concat([actuals_df, forecasts_df])\n",
        "         \n",
        "        wandb.join()\n",
        "        \n",
        "        return output_df_m4m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca_gjBIc0XBI"
      },
      "source": [
        "#Run experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad-MTPx30a2L"
      },
      "source": [
        "Setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQaCAmki0c0c"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULpeBbAg0el8"
      },
      "outputs": [],
      "source": [
        "wandb_project_name = 'M4_NBEATS_MPS_DEC_TUNING'\n",
        "job_type_name = 'validation_full'\n",
        "# one of:\n",
        "# - 'test', \n",
        "# - 'validation_full' --> e.g., for lambda value tuning\n",
        "# - 'validation_earlystop' --> for specifying number of epochs\n",
        "\n",
        "# Setup used for M4 data set #M3\n",
        "hyperparameter_defaults = dict()\n",
        "hyperparameter_defaults['epochs'] = 155 #4000\n",
        "hyperparameter_defaults['batch_size'] = 512\n",
        "hyperparameter_defaults['nb_blocks_per_stack'] = 1\n",
        "hyperparameter_defaults['thetas_dims'] = 128 \n",
        "hyperparameter_defaults['n_stacks'] = 10 \n",
        "hyperparameter_defaults['share_weights_in_stack'] = False\n",
        "hyperparameter_defaults[\"backcast_length_multiplier\"] = 4 #6\n",
        "hyperparameter_defaults['hidden_layer_units'] = 128 \n",
        "hyperparameter_defaults['share_thetas'] = False\n",
        "hyperparameter_defaults[\"dropout\"] = False\n",
        "hyperparameter_defaults[\"dropout_p\"] = 0.0\n",
        "hyperparameter_defaults[\"neg_slope\"] = 0.00\n",
        "hyperparameter_defaults['learning_rate'] = 0.001 #0.00001\n",
        "hyperparameter_defaults[\"weight_decay\"] = 0.00\n",
        "hyperparameter_defaults[\"LH\"] = 10 #2\n",
        "hyperparameter_defaults[\"rndseed\"] = 2000\n",
        "hyperparameter_defaults[\"loss_function\"] = 1 # 1 == RMSSE / 2 == RMSSE_m / 3 == SMAPE / 4 == MAPE\n",
        "hyperparameter_defaults[\"shifts\"] = 5\n",
        "hyperparameter_defaults['patience'] = 200 # Only affects 'validation_earlystop' runs\n",
        "hyperparameter_defaults['lambda'] = 0.15 # Weight of forecast stability loss in loss_combined\n",
        "# Note that lambda is defined in the code as the proportion of stability in total loss (relative terms)\n",
        "# In the paper, lambda is defined in absolute terms\n",
        "# So: lambda_paper = lambda_code/(1-lambda_code)\n",
        "hyperparameter_defaults['alpha'] = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldwB_-hM0g9p"
      },
      "outputs": [],
      "source": [
        "if job_type_name == 'test':\n",
        "    is_val = False\n",
        "    do_earlystop = False\n",
        "    m4_train, m4_eval = valset_m4m, testset_m4m\n",
        "elif job_type_name == 'validation_full':\n",
        "    is_val = True\n",
        "    do_earlystop = False\n",
        "    m4_train, m4_eval = trainset_m4m, valset_m4m\n",
        "elif job_type_name == 'validation_earlystop':\n",
        "    is_val = True\n",
        "    do_earlystop = True\n",
        "    m4_train, m4_eval = trainset_m4m, valset_m4m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlhHwK_30jeT"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uK_1eIa0lBS"
      },
      "source": [
        "Run:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WoMOTKw0mZi"
      },
      "outputs": [],
      "source": [
        "def sweep_function():\n",
        "    wandb.init(config = hyperparameter_defaults,\n",
        "               project = wandb_project_name,\n",
        "               job_type = job_type_name)\n",
        "    config = wandb.config\n",
        "    run_name = wandb.run.name\n",
        "    \n",
        "\n",
        "   \n",
        "    # Initialize model\n",
        "    StableNBeats_model = StableNBeatsLearner(device, 6, config)\n",
        "    # Train & evaluate\n",
        "    forecasts_df_m4m = StableNBeats_model.train_net(m4_train, m4_eval, 13, is_val, do_earlystop)\n",
        "    # Save forecasts\n",
        "    #forecasts_df_m4m.to_csv('m4m_nbeats_stability_' + job_type_name + '_' + run_name + '.csv', index = False)\n",
        "    #forecasts_df_m4m.to_csv('/content/drive/MyDrive/Thesis/code-saved' + job_type_name + '_' + run_name + '.csv', index = False)\n",
        "    #print(\"forecasts saved to googledrive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz-3CvF90oxb"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"sweep\",\n",
        "    \"method\": \"grid\",\n",
        "    \"parameters\": {\n",
        "        \"rndseed\": {\n",
        "            \"values\": [1000]\n",
        "        },\n",
        "        \"lambda\":{\n",
        "            \"values\": [0.125]\n",
        "        },\n",
        "        \"alpha\": {\n",
        "            \"values\": [0.9]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, project = wandb_project_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb14nct90sFw"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, function = sweep_function)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}